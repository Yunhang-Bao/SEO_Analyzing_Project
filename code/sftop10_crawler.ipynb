{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/baoyunhang/Desktop/Hackthon/.venv/lib/python3.13/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/baoyunhang/Desktop/Hackthon/.venv/lib/python3.13/site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/baoyunhang/Desktop/Hackthon/.venv/lib/python3.13/site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/baoyunhang/Desktop/Hackthon/.venv/lib/python3.13/site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install requests\n",
    "#!pip install pandas\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/baoyunhang/Desktop/Hackthon/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing in /Users/baoyunhang/Desktop/Hackthon/.venv/lib/python3.13/site-packages (3.7.4.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#! /usr/local/bin/python3 -m pip install requests pandas beautifulsoup4\n",
    "! pip install typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "import socket\n",
    "import http.client\n",
    "from playwright.async_api import async_playwright, Browser, Page\n",
    "delay = random.uniform(2, 5)  # 随机延时 2-5 秒\n",
    "from typing import Dict, Optional\n",
    "time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_seo_signals(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status() \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        #Title tags\n",
    "        title = soup.title.string if soup.title else \"No Title Found\"\n",
    "\n",
    "        # Meta Description\n",
    "        meta_description = \"\"\n",
    "        meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta:\n",
    "            meta_description = meta.get(\"content\", \"\")\n",
    "\n",
    "        # H1 tag\n",
    "        h1_tags = [h1.get_text(strip=True) for h1 in soup.find_all(\"h1\")]\n",
    "\n",
    "        #sitemap\n",
    "        sitemap_exists = soup.find(\"a\", href=True, text=lambda x: \"sitemap\" in x.lower()) is not None\n",
    "\n",
    "        #\n",
    "        images = soup.find_all(\"img\")\n",
    "        image_alts = [img.get(\"alt\", \"No Alt\") for img in images]\n",
    "\n",
    "        # HTTPS\n",
    "        https_used = url.startswith(\"https\")\n",
    "\n",
    "        # Schema Markup\n",
    "        structured_data = bool(soup.find(\"script\", type=\"application/ld+json\"))\n",
    "\n",
    "        # mobile friendly\n",
    "        viewport = soup.find(\"meta\", attrs={\"name\": \"viewport\"})\n",
    "        mobile_friendly = viewport is not None\n",
    "\n",
    "        # loading time\n",
    "        load_time = response.elapsed.total_seconds()\n",
    "\n",
    "        # Canonical tag\n",
    "        canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "        canonical_url = canonical_tag['href'] if canonical_tag else \"No Canonical Tag\"\n",
    "\n",
    "        # robots meta tag\n",
    "        robots_meta = \"\"\n",
    "        robots = soup.find(\"meta\", attrs={\"name\": \"robots\"})\n",
    "        if robots:\n",
    "            robots_meta = robots.get(\"content\", \"\")\n",
    "\n",
    "        \n",
    "        internal_links = []\n",
    "        external_links = []\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag[\"href\"]\n",
    "            if url in href or href.startswith(\"/\"):\n",
    "                internal_links.append(href)\n",
    "            else:\n",
    "                external_links.append(href)\n",
    "\n",
    "       \n",
    "        body_content = soup.body.get_text(separator=\" \", strip=True) if soup.body else \"\"\n",
    "        word_count = len(body_content.split())\n",
    "\n",
    "        \n",
    "        keyword = \"law\"\n",
    "        keyword_density = body_content.lower().count(keyword) / len(body_content.split()) if body_content else 0\n",
    "\n",
    "        \n",
    "        favicon = soup.find(\"link\", rel=\"icon\") or soup.find(\"link\", rel=\"shortcut icon\")\n",
    "        favicon_exists = favicon[\"href\"] if favicon else \"No Favicon\"\n",
    "\n",
    "        \n",
    "        og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "        og_description = soup.find(\"meta\", property=\"og:description\")\n",
    "        og_image = soup.find(\"meta\", property=\"og:image\")\n",
    "        social_tags = {\n",
    "            \"OG Title\": og_title[\"content\"] if og_title else \"No OG Title\",\n",
    "            \"OG Description\": og_description[\"content\"] if og_description else \"No OG Description\",\n",
    "            \"OG Image\": og_image[\"content\"] if og_image else \"No OG Image\",\n",
    "        }\n",
    "\n",
    "        \n",
    "        broken_links = 0\n",
    "        for link in external_links + internal_links:\n",
    "            try:\n",
    "                link_response = requests.head(link, headers=headers, timeout=5)\n",
    "                if link_response.status_code >= 400:\n",
    "                    broken_links += 1\n",
    "            except:\n",
    "                broken_links += 1\n",
    "\n",
    "        \n",
    "        core_web_vitals = {\"LCP\": \"N/A\", \"FID\": \"N/A\", \"CLS\": \"N/A\"}\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"URL\": url,\n",
    "            \"Title\": title,\n",
    "            \"Meta Description\": meta_description,\n",
    "            \"H1 Tags\": h1_tags,\n",
    "            \"Sitemap Exists\": sitemap_exists,\n",
    "            \"HTTPS Used\": https_used,\n",
    "            \"Image Alts\": image_alts,\n",
    "            \"Structured Data\": structured_data,\n",
    "            \"Mobile Friendly\": mobile_friendly,\n",
    "            \"Load Time (s)\": load_time,\n",
    "            \"Canonical Tag\": canonical_url,\n",
    "            \"Robots Meta Tags\": robots_meta,\n",
    "            \"Internal Links\": len(internal_links),\n",
    "            \"External Links\": len(external_links),\n",
    "            \"Word Count\": word_count,\n",
    "            \"Keyword Density\": keyword_density,\n",
    "            \"Favicon Exists\": favicon_exists,\n",
    "            \"OG Tags\": social_tags,\n",
    "            \"Broken Links\": broken_links,\n",
    "            \"Core Web Vitals\": core_web_vitals,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"URL\": url, \"Error\": str(e)}\n",
    "\n",
    "    \n",
    "def read_urls_from_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    urls_to_scrape = df[df['in_progress'] == 0].head(10)\n",
    "    return urls_to_scrape, df\n",
    "\n",
    "\n",
    "def update_progress(df, index, output_file):\n",
    "    df.at[index, 'in_progress'] = 1\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file = r\"/Users/baoyunhang/Desktop/parcaticum/export_url_file.csv\"\n",
    "    output_file = r\"/Users/baoyunhang/Desktop/parcaticum/export_url_file_updated.csv\"\n",
    "\n",
    "    try:\n",
    "        urls_to_scrape, df = read_urls_from_file(input_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading input file: {e}\")\n",
    "        return\n",
    "\n",
    "    if urls_to_scrape.empty:\n",
    "        print(\"No URLs to process. All URLs are marked as completed.\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    delay = 2\n",
    "\n",
    "    for index, row in urls_to_scrape.iterrows():\n",
    "        url = row['link']\n",
    "        print(f\"Scraping: {url}\")\n",
    "        seo_data = scrape_seo_signals(url)\n",
    "        results.append(seo_data)\n",
    "\n",
    "        update_progress(df, index, output_file)\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(r\"/Users/baoyunhang/Desktop/parcaticum/export_url_file.csv\", index=False)\n",
    "    print(\"All URLs processed. Results saved to seo_signals_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncSEOScraper:\n",
    "    def __init__(self):\n",
    "        self.playwright = None\n",
    "        self.browser: Optional[Browser] = None\n",
    "        self.context = None\n",
    "\n",
    "    async def init_browser(self):\n",
    "        \"\"\"初始化浏览器\"\"\"\n",
    "        try:\n",
    "            if self.browser:\n",
    "                await self.browser.close()\n",
    "            \n",
    "            self.playwright = await async_playwright().start()\n",
    "            self.browser = await self.playwright.chromium.launch(\n",
    "                headless=True,\n",
    "                args=[\n",
    "                    '--disable-blink-features=AutomationControlled',\n",
    "                    '--disable-dev-shm-usage',\n",
    "                    '--no-sandbox',\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.context = await self.browser.new_context(\n",
    "                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "                viewport={'width': 1920, 'height': 1080}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing browser: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def get_performance_metrics(self, page: Page):\n",
    "        \"\"\"\"Obtain performance indicators\"\"\"\n",
    "        try:\n",
    "            metrics = await page.evaluate(\"\"\"() => {\n",
    "                const timing = window.performance.timing;\n",
    "                const navigationStart = timing.navigationStart;\n",
    "                return {\n",
    "                    'Load Time': (timing.loadEventEnd - navigationStart) / 1000,\n",
    "                    'DOM Content Load': (timing.domContentLoadedEventEnd - navigationStart) / 1000,\n",
    "                    'First Paint': performance.getEntriesByType('paint')[0]?.startTime / 1000 || 'N/A'\n",
    "                }\n",
    "            }\"\"\")\n",
    "            return metrics\n",
    "        except:\n",
    "            return {'Load Time': 'N/A', 'DOM Content Load': 'N/A', 'First Paint': 'N/A'}\n",
    "\n",
    "    async def check_link_status(self, page: Page, link: str, base_url: str) -> bool:\n",
    "        \"\"\"Check link status\"\"\"\n",
    "        try:\n",
    "            full_url = urljoin(base_url, link)\n",
    "            response = await page.context.request.head(full_url)\n",
    "            return response.ok\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    async def scrape_url(self, url: str, retry_count: int = 3) -> Dict:\n",
    "        \"\"\"crawl single url signal\"\"\"\n",
    "        for attempt in range(retry_count):\n",
    "            page = None\n",
    "            try:\n",
    "                if not self.browser:\n",
    "                    await self.init_browser()\n",
    "                \n",
    "                page = await self.context.new_page()\n",
    "                await page.set_extra_http_headers({\n",
    "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                    'Accept-Language': 'en-US,en;q=0.5',\n",
    "                    'Accept-Encoding': 'gzip, deflate, br',\n",
    "                    'DNT': '1'\n",
    "                })\n",
    "                \n",
    "                start_time = time.time()\n",
    "                response = await page.goto(url, wait_until='networkidle')\n",
    "                load_time = time.time() - start_time\n",
    "                \n",
    "                if response and response.status == 403:\n",
    "                    raise Exception(\"403 Forbidden\")\n",
    "                \n",
    "                # wait page load\n",
    "                await page.wait_for_load_state('networkidle')\n",
    "                content = await page.content()\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                # basic seo information\n",
    "                title = soup.title.string.strip() if soup.title else \"No Title Found\"\n",
    "                meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                meta_description = meta.get(\"content\", \"\").strip() if meta else \"No Description Found\"\n",
    "                \n",
    "                # H1tag\n",
    "                h1_tags = [h1.get_text(strip=True) for h1 in soup.find_all(\"h1\")]\n",
    "                \n",
    "                # picture alt text\n",
    "                images = soup.find_all(\"img\")\n",
    "                image_alts = [img.get(\"alt\", \"No Alt\") for img in images]\n",
    "                \n",
    "                # Schema sign\n",
    "                structured_data = bool(soup.find(\"script\", type=\"application/ld+json\"))\n",
    "                \n",
    "                # mobile_friendly\n",
    "                viewport = soup.find(\"meta\", attrs={\"name\": \"viewport\"})\n",
    "                mobile_friendly = viewport is not None\n",
    "                \n",
    "                # tag\n",
    "                canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "                canonical_url = canonical_tag['href'] if canonical_tag else \"No Canonical Tag\"\n",
    "                \n",
    "                # Robots meta标签\n",
    "                robots = soup.find(\"meta\", attrs={\"name\": \"robots\"})\n",
    "                robots_meta = robots.get(\"content\", \"\") if robots else \"No Robots Meta\"\n",
    "                \n",
    "                # link analysis\n",
    "                internal_links = []\n",
    "                external_links = []\n",
    "                domain = urlparse(url).netloc\n",
    "                for a_tag in soup.find_all(\"a\", href=True):\n",
    "                    href = a_tag[\"href\"]\n",
    "                    if domain in href or href.startswith(\"/\"):\n",
    "                        internal_links.append(href)\n",
    "                    else:\n",
    "                        external_links.append(href)\n",
    "                \n",
    "                # content analysis\n",
    "                body_content = soup.body.get_text(separator=\" \", strip=True) if soup.body else \"\"\n",
    "                word_count = len(body_content.split())\n",
    "                \n",
    "                # keywords density\n",
    "                keyword = \"lawyer\"  # 针对律师网站\n",
    "                keyword_density = body_content.lower().count(keyword) / word_count if word_count > 0 else 0\n",
    "                \n",
    "                # social tag\n",
    "                og_title = soup.find(\"meta\", property=\"og:title\")\n",
    "                og_description = soup.find(\"meta\", property=\"og:description\")\n",
    "                og_image = soup.find(\"meta\", property=\"og:image\")\n",
    "                \n",
    "                # performance metric\n",
    "                performance_metrics = await self.get_performance_metrics(page)\n",
    "                \n",
    "                return {\n",
    "                    \"URL\": url,\n",
    "                    \"Title\": title,\n",
    "                    \"Meta Description\": meta_description,\n",
    "                    \"H1 Tags\": h1_tags,\n",
    "                    \"HTTPS Used\": url.startswith(\"https\"),\n",
    "                    \"Image Alts Count\": len([alt for alt in image_alts if alt != \"No Alt\"]),\n",
    "                    \"Total Images\": len(images),\n",
    "                    \"Structured Data Present\": structured_data,\n",
    "                    \"Mobile Friendly\": mobile_friendly,\n",
    "                    \"Load Time (s)\": performance_metrics['Load Time'],\n",
    "                    \"DOM Content Load (s)\": performance_metrics['DOM Content Load'],\n",
    "                    \"First Paint (s)\": performance_metrics['First Paint'],\n",
    "                    \"Canonical Tag\": canonical_url,\n",
    "                    \"Robots Meta\": robots_meta,\n",
    "                    \"Internal Links Count\": len(internal_links),\n",
    "                    \"External Links Count\": len(external_links),\n",
    "                    \"Word Count\": word_count,\n",
    "                    \"Keyword Density\": f\"{keyword_density:.2%}\",\n",
    "                    \"Has OG Title\": og_title is not None,\n",
    "                    \"Has OG Description\": og_description is not None,\n",
    "                    \"Has OG Image\": og_image is not None,\n",
    "                    \"Status\": \"Success\"\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
    "                if attempt == retry_count - 1:\n",
    "                    return {\n",
    "                        \"URL\": url,\n",
    "                        \"Status\": f\"Failed: {str(e)}\"\n",
    "                    }\n",
    "                await asyncio.sleep(random.uniform(20, 30))\n",
    "                await self.init_browser()\n",
    "            finally:\n",
    "                if page:\n",
    "                    await page.close()\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"关闭浏览器和playwright\"\"\"\n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "async def run_scraper():\n",
    "    \"\"\"using async run_scraper\"\"\"\n",
    "    input_file = r\"/Users/baoyunhang/Downloads/top_10_results_filtered.csv\"\n",
    "    output_file = r\"/Users/baoyunhang/Downloads/top_10_results_filtered_updated.csv\"\n",
    "    results_file = r\"/Users/baoyunhang/Desktop/parcaticum/sf_top10_seo_signals_results.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        if 'in_progress' not in df.columns:\n",
    "            df['in_progress'] = 0\n",
    "        \n",
    "        urls_to_scrape = df[df['in_progress'] == 0].head(1)\n",
    "        \n",
    "        if urls_to_scrape.empty:\n",
    "            logging.info(\"No URLs to process. All URLs are marked as completed.\")\n",
    "            return\n",
    "        \n",
    "        scraper = AsyncSEOScraper()\n",
    "        results = []\n",
    "        \n",
    "        try:\n",
    "            for index, row in urls_to_scrape.iterrows():\n",
    "                url = row['link']\n",
    "                logging.info(f\"Processing: {url}\")\n",
    "                \n",
    "                result = await scraper.scrape_url(url)\n",
    "                results.append(result)\n",
    "                \n",
    "                df.at[index, 'in_progress'] = 1\n",
    "                df.to_csv(output_file, index=False)\n",
    "                \n",
    "                await asyncio.sleep(random.uniform(15, 25))\n",
    "            \n",
    "            results_df = pd.DataFrame(results)\n",
    "            \n",
    "            try:\n",
    "                existing_results = pd.read_csv(results_file)\n",
    "                combined_results = pd.concat([existing_results, results_df], ignore_index=True)\n",
    "                combined_results.to_csv(results_file, index=False)\n",
    "            except FileNotFoundError:\n",
    "                results_df.to_csv(results_file, index=False)\n",
    "            \n",
    "        finally:\n",
    "            await scraper.close()\n",
    "        \n",
    "        logging.info(\"Processing completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in main: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    asyncio.get_event_loop().run_until_complete(run_scraper())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SEO scraper...\n",
      "Log file: scraper_20250510_101039.log\n",
      "2025-05-10 10:10:39,980 - INFO - Reading input file: /Users/baoyunhang/Downloads/top_10_results_filtered.csv\n",
      "2025-05-10 10:10:39,995 - INFO - Successfully read 5877 rows from input file\n",
      "2025-05-10 10:10:39,996 - INFO - Added 'in_progress' column to DataFrame\n",
      "2025-05-10 10:10:39,996 - INFO - Found 5877 URLs to process\n",
      "2025-05-10 10:10:39,997 - INFO - Processing URL 1/5877 (0.02%): https://www.tslo.com/real-estate-law/commercial-leasing/\n",
      "2025-05-10 10:10:41,986 - INFO - Successfully scraped data for: https://www.tslo.com/real-estate-law/commercial-leasing/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLog file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Hackthon/.venv/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Hackthon/.venv/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:548\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    546\u001b[39m ready = []\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10 10:10:52,119 - INFO - Processing URL 2/5877 (0.03%): https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/\n",
      "2025-05-10 10:10:52,136 - ERROR - Attempt 1 failed for https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:11:18,518 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:11:18,520 - ERROR - Error processing URL https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:11:18,521 - INFO - Processing URL 3/5877 (0.05%): https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/\n",
      "2025-05-10 10:11:18,528 - ERROR - Attempt 1 failed for https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:14:43,918 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:14:43,930 - ERROR - Error processing URL https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:14:43,971 - INFO - Processing URL 4/5877 (0.07%): https://lccrsf.org/get-assistance/legal-services-for-entrepreneurs/commercial-tenant-legal-assistance/\n",
      "2025-05-10 10:14:44,020 - ERROR - Attempt 1 failed for https://lccrsf.org/get-assistance/legal-services-for-entrepreneurs/commercial-tenant-legal-assistance/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:07,052 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:15:07,053 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:07,053 - ERROR - Error processing URL https://lccrsf.org/get-assistance/legal-services-for-entrepreneurs/commercial-tenant-legal-assistance/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:07,054 - INFO - Processing URL 5/5877 (0.09%): https://www.shaperolawfirm.com/practice-area/san-francisco-commercial-real-estate-lawyer/\n",
      "2025-05-10 10:15:07,062 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:15:07,063 - ERROR - Attempt 1 failed for https://www.shaperolawfirm.com/practice-area/san-francisco-commercial-real-estate-lawyer/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:34,585 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:15:34,586 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:34,586 - ERROR - Error processing URL https://www.shaperolawfirm.com/practice-area/san-francisco-commercial-real-estate-lawyer/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:15:34,586 - INFO - Processing URL 6/5877 (0.10%): https://www.sfbar.org/lris/business-lawyers/\n",
      "2025-05-10 10:15:34,594 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:15:34,594 - ERROR - Attempt 1 failed for https://www.sfbar.org/lris/business-lawyers/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:02,322 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:02,323 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:02,324 - ERROR - Error processing URL https://www.sfbar.org/lris/business-lawyers/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:02,325 - INFO - Processing URL 7/5877 (0.12%): https://www.jacobsonlawsf.com/services\n",
      "2025-05-10 10:16:02,332 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:02,333 - ERROR - Attempt 1 failed for https://www.jacobsonlawsf.com/services: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:29,960 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:29,963 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:29,963 - ERROR - Error processing URL https://www.jacobsonlawsf.com/services: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:29,964 - INFO - Processing URL 8/5877 (0.14%): https://www.jmbm.com/commercial-lease-litigation.html\n",
      "2025-05-10 10:16:29,971 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:29,971 - ERROR - Attempt 1 failed for https://www.jmbm.com/commercial-lease-litigation.html: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:58,108 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:58,110 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:58,110 - ERROR - Error processing URL https://www.jmbm.com/commercial-lease-litigation.html: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:16:58,111 - INFO - Processing URL 9/5877 (0.15%): https://samlaw.net/practice-areas/landlord-tenant-law/commercial-landlord-tenant-disputes/\n",
      "2025-05-10 10:16:58,118 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:16:58,118 - ERROR - Attempt 1 failed for https://samlaw.net/practice-areas/landlord-tenant-law/commercial-landlord-tenant-disputes/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:26,410 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:17:26,411 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:26,411 - ERROR - Error processing URL https://samlaw.net/practice-areas/landlord-tenant-law/commercial-landlord-tenant-disputes/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:26,412 - INFO - Processing URL 10/5877 (0.17%): https://www.sfbar.org/lris/real-estate-lawyers/\n",
      "2025-05-10 10:17:26,419 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:17:26,419 - ERROR - Attempt 1 failed for https://www.sfbar.org/lris/real-estate-lawyers/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:46,922 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:17:46,924 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:46,924 - ERROR - Error processing URL https://www.sfbar.org/lris/real-estate-lawyers/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:17:46,949 - INFO - Saved batch results. Total processed: 10/5877\n",
      "\n",
      "Progress: 10/5877 (0.17%)\n",
      "Latest batch results preview:\n",
      "                                                 URL  \\\n",
      "0  https://www.tslo.com/real-estate-law/commercia...   \n",
      "1  https://www.eastbaybusinesslawyer.com/commerci...   \n",
      "2  https://www.zfplaw.com/real-estate-transaction...   \n",
      "3  https://lccrsf.org/get-assistance/legal-servic...   \n",
      "4  https://www.shaperolawfirm.com/practice-area/s...   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Commercial Lease Lawyer | Commercial Lease Dis...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                              Status  \n",
      "0                                            Success  \n",
      "1  Failed: Browser.close: Connection closed while...  \n",
      "2  Failed: Browser.close: Connection closed while...  \n",
      "3  Failed: Browser.close: Connection closed while...  \n",
      "4  Failed: Browser.close: Connection closed while...  \n",
      "2025-05-10 10:18:14,931 - INFO - Processing URL 11/5877 (0.19%): https://www.coblentzlaw.com/practice-area/real-estate/\n",
      "2025-05-10 10:18:14,943 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:18:14,943 - ERROR - Attempt 1 failed for https://www.coblentzlaw.com/practice-area/real-estate/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:35,205 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:18:35,206 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:35,207 - ERROR - Error processing URL https://www.coblentzlaw.com/practice-area/real-estate/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:35,207 - INFO - Processing URL 12/5877 (0.20%): https://www.zfplaw.com/real-estate-transactions/\n",
      "2025-05-10 10:18:35,214 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:18:35,214 - ERROR - Attempt 1 failed for https://www.zfplaw.com/real-estate-transactions/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:59,454 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:18:59,455 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:59,456 - ERROR - Error processing URL https://www.zfplaw.com/real-estate-transactions/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:18:59,456 - INFO - Processing URL 13/5877 (0.22%): https://www.tslo.com/real-estate-law/\n",
      "2025-05-10 10:18:59,463 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:18:59,464 - ERROR - Attempt 1 failed for https://www.tslo.com/real-estate-law/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:29,179 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:19:29,181 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:29,182 - ERROR - Error processing URL https://www.tslo.com/real-estate-law/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:29,182 - INFO - Processing URL 14/5877 (0.24%): https://www.severson.com/practice-category/real-estate/\n",
      "2025-05-10 10:19:29,189 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:19:29,189 - ERROR - Attempt 1 failed for https://www.severson.com/practice-category/real-estate/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:58,218 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:19:58,219 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:58,219 - ERROR - Error processing URL https://www.severson.com/practice-category/real-estate/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:19:58,220 - INFO - Processing URL 15/5877 (0.26%): https://www.woodlitigation.com/practice-areas/real-estate/\n",
      "2025-05-10 10:19:58,225 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:19:58,225 - ERROR - Attempt 1 failed for https://www.woodlitigation.com/practice-areas/real-estate/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:19,251 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:20:19,282 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:19,282 - ERROR - Error processing URL https://www.woodlitigation.com/practice-areas/real-estate/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:19,291 - INFO - Processing URL 16/5877 (0.27%): https://www.hklaw.com/en/offices/san-francisco\n",
      "2025-05-10 10:20:19,307 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:20:19,319 - ERROR - Attempt 1 failed for https://www.hklaw.com/en/offices/san-francisco: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:41,258 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:20:41,259 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:41,259 - ERROR - Error processing URL https://www.hklaw.com/en/offices/san-francisco: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:20:41,260 - INFO - Processing URL 17/5877 (0.29%): https://www.gibsondunn.com/office/san-francisco/\n",
      "2025-05-10 10:20:41,267 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:20:41,268 - ERROR - Attempt 1 failed for https://www.gibsondunn.com/office/san-francisco/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:02,850 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:02,851 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:02,852 - ERROR - Error processing URL https://www.gibsondunn.com/office/san-francisco/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:02,853 - INFO - Processing URL 18/5877 (0.31%): https://www.tslo.com/real-estate-law/commercial-leasing/\n",
      "2025-05-10 10:21:02,859 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:02,859 - ERROR - Attempt 1 failed for https://www.tslo.com/real-estate-law/commercial-leasing/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:27,696 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:27,697 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:27,698 - ERROR - Error processing URL https://www.tslo.com/real-estate-law/commercial-leasing/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:27,699 - INFO - Processing URL 19/5877 (0.32%): https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/\n",
      "2025-05-10 10:21:27,706 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:27,707 - ERROR - Attempt 1 failed for https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:47,788 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:47,789 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:47,789 - ERROR - Error processing URL https://www.eastbaybusinesslawyer.com/commercial-lease-review-negotiation-lawyer/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:21:47,790 - INFO - Processing URL 20/5877 (0.34%): https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/\n",
      "2025-05-10 10:21:47,796 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:21:47,797 - ERROR - Attempt 1 failed for https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/: BrowserContext.new_page: Connection closed while reading from the driver\n",
      "2025-05-10 10:22:15,885 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:22:15,886 - ERROR - Error initializing browser: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:22:15,887 - ERROR - Error processing URL https://www.zfplaw.com/real-estate-transactions/rental-agreements-and-leases/: Browser.close: Connection closed while reading from the driver\n",
      "2025-05-10 10:22:15,912 - INFO - Saved batch results. Total processed: 20/5877\n",
      "\n",
      "Progress: 20/5877 (0.34%)\n",
      "Latest batch results preview:\n",
      "2025-05-10 10:22:39,288 - WARNING - pipe closed by peer or os.write(pipe, data) raised exception.\n",
      "2025-05-10 10:22:39,289 - ERROR - An error occurred in main: Browser.close: Connection closed while reading from the driver\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright, Browser, Page\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Optional, Dict\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# setting more consice log\n",
    "log_file = f'scraper_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# activate nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#[keep former AsyncSEOScraper class code unchanged...]\n",
    "\n",
    "async def run_scraper():\n",
    "    \n",
    "    input_file = r\"/Users/baoyunhang/Downloads/top_10_results_filtered.csv\"\n",
    "    output_file = r\"/Users/baoyunhang/Downloads/top_10_results_filtered_updated.csv\"\n",
    "    results_file = r\"/Users/baoyunhang/Desktop/parcaticum/sf_top10_seo_signals_results.csv\"\n",
    "    \n",
    "    try:\n",
    "        # read input_file\n",
    "        logging.info(f\"Reading input file: {input_file}\")\n",
    "        df = pd.read_csv(input_file)\n",
    "        total_urls = len(df)\n",
    "        logging.info(f\"Successfully read {total_urls} rows from input file\")\n",
    "        \n",
    "        if 'in_progress' not in df.columns:\n",
    "            df['in_progress'] = 0\n",
    "            logging.info(\"Added 'in_progress' column to DataFrame\")\n",
    "        \n",
    "        # obtain all undisposed data\n",
    "        urls_to_scrape = df[df['in_progress'] == 0]\n",
    "        remaining_urls = len(urls_to_scrape)\n",
    "        logging.info(f\"Found {remaining_urls} URLs to process\")\n",
    "        \n",
    "        if urls_to_scrape.empty:\n",
    "            logging.info(\"No URLs to process. All URLs are marked as completed.\")\n",
    "            return\n",
    "        \n",
    "        scraper = AsyncSEOScraper()\n",
    "        batch_size = 10  # every batch URL amount\n",
    "        total_processed = 0\n",
    "        \n",
    "        try:\n",
    "            # dispose URL in batches\n",
    "            for start_idx in range(0, len(urls_to_scrape), batch_size):\n",
    "                batch = urls_to_scrape.iloc[start_idx:start_idx + batch_size]\n",
    "                results = []\n",
    "                \n",
    "                for index, row in batch.iterrows():\n",
    "                    url = row['link']\n",
    "                    total_processed += 1\n",
    "                    \n",
    "                    try:\n",
    "                        logging.info(f\"Processing URL {total_processed}/{remaining_urls} ({(total_processed/remaining_urls)*100:.2f}%): {url}\")\n",
    "                        \n",
    "                        # crawl\n",
    "                        result = await scraper.scrape_url(url)\n",
    "                        results.append(result)\n",
    "                        logging.info(f\"Successfully scraped data for: {url}\")\n",
    "                        \n",
    "                        # update\n",
    "                        df.at[index, 'in_progress'] = 1\n",
    "                        df.to_csv(output_file, index=False)\n",
    "                        \n",
    "                        #random wait\n",
    "                        await asyncio.sleep(random.uniform(10, 15))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing URL {url}: {str(e)}\")\n",
    "                        results.append({\n",
    "                            \"URL\": url,\n",
    "                            \"Status\": f\"Failed: {str(e)}\"\n",
    "                        })\n",
    "                \n",
    "                # save results of every batch\n",
    "                results_df = pd.DataFrame(results)\n",
    "                try:\n",
    "                    if os.path.exists(results_file):\n",
    "                        existing_results = pd.read_csv(results_file)\n",
    "                        combined_results = pd.concat([existing_results, results_df], ignore_index=True)\n",
    "                        combined_results.to_csv(results_file, index=False)\n",
    "                    else:\n",
    "                        results_df.to_csv(results_file, index=False)\n",
    "                    \n",
    "                    logging.info(f\"Saved batch results. Total processed: {total_processed}/{remaining_urls}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error saving results file: {str(e)}\")\n",
    "                \n",
    "                # Wait extra after each batch to avoid being blocked\n",
    "                await asyncio.sleep(random.uniform(20, 30))\n",
    "                \n",
    "                # print status\n",
    "                print(f\"\\nProgress: {total_processed}/{remaining_urls} ({(total_processed/remaining_urls)*100:.2f}%)\")\n",
    "                print(f\"Latest batch results preview:\")\n",
    "                if not results_df.empty:\n",
    "                    print(results_df[['URL', 'Title', 'Status']].head())\n",
    "            \n",
    "        finally:\n",
    "            await scraper.close()\n",
    "        \n",
    "        logging.info(\"Processing completed successfully\")\n",
    "        print(\"\\nScraping completed!\")\n",
    "        print(f\"Total URLs processed: {total_processed}\")\n",
    "        print(f\"Results saved to: {results_file}\")\n",
    "        print(f\"Log file: {log_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting SEO scraper...\")\n",
    "    print(f\"Log file: {log_file}\")\n",
    "    try:\n",
    "        asyncio.get_event_loop().run_until_complete(run_scraper())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        print(f\"Check {log_file} for error details\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
